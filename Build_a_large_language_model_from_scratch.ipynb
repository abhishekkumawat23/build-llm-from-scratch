{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyO+TQvlyokWux5IOkoFi8xn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekkumawat23/build-llm-from-scratch/blob/main/Build_a_large_language_model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Large Language Model from Scratch\n",
        "\n",
        "This notebook follows the [Build a Large Language Model (From Scratch)](https://sebastianraschka.com/llms-from-scratch/) book to build a large language model from scratch.\n",
        "\n",
        "Wherever needed, the notebook explains the relevant theory and underlying concepts, focusing on learning through hands-on implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## What is an LLM (Large Language Model)?\n",
        "\n",
        "**Large Language Models (LLMs)** can be of various types, but the most common ones are **Generative LLMs**. A Generative LLM is a deep neural network model that, when given a piece of text (whether a word, line, or paragraph), generates the next word (or token, to be precise) that should follow that text.\n",
        "\n",
        "### Example\n",
        "\n",
        "- Given `The cat sat`, the LLM might generate `on`\n",
        "- Given `The cat sat on`, the LLM might generate `the`\n",
        "- Given `The cat sat on the`, the LLM might generate `mat`\n",
        "\n",
        "Since the model can generate the next word after any given text, if we pass `The cat sat` and call the model multiple times—each time appending the word it generated in the previous iteration—we can generate a significant chunk of text. For example, starting with just `The cat sat`, we can generate `The cat sat on the mat and started playing.`\n",
        "\n",
        "### How It Works\n",
        "\n",
        "- `The cat sat` → `The cat sat on`\n",
        "- `The cat sat on` → `The cat sat on the`\n",
        "- `The cat sat on the` → `The cat sat on the mat`\n",
        "- `The cat sat on the mat` → `The cat sat on the mat and`\n",
        "- `The cat sat on the mat and` → `The cat sat on the mat and started`\n",
        "- `The cat sat on the mat and started` → `The cat sat on the mat and started playing`\n",
        "\n",
        "End users don't need to call the model repeatedly after each word—the LLM wraps this logic internally, calling the underlying model multiple times and appending each generated word to the next iteration. This is why such LLMs are also called **Auto-Regressive** models.\n",
        "\n",
        "---\n",
        "\n",
        "### TODO\n",
        "Introduce additional foundational concepts such as:\n",
        "- Deep neural networks\n",
        "- Transformer layers\n",
        "- Attention mechanisms\n",
        "- Encoder-only, decoder-only, and encoder-decoder architectures\n",
        "- Tokenizers\n",
        "- Embeddings\n",
        "- Difference between inference and training\n",
        "- During training, the entire input is processed at once, while during inference, text is generated one token at a time\n",
        "- KV caching\n",
        "\n",
        "*Note: Some of these concepts may be introduced at later stages in the notebook.*"
      ],
      "metadata": {
        "id": "JXR6lT8iikkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Text Processing\n",
        "\n",
        "## What is an Embedding?\n",
        "\n",
        "LLMs don't understand text the way humans do—they understand numbers. Therefore, we need to:\n",
        "- Convert the input text to a numerical representation before passing it to the LLM\n",
        "- When the LLM predicts the next word, it outputs a numerical representation that we need to convert back to text\n",
        "\n",
        "This numerical representation of text is called an **embedding**. An embedding is not a single integer or float, but rather a vector of floats. The length of this vector depends on the LLM architecture and can be 32, 64, 768, 4096, or more. The length of the embedding vector is called the **embedding dimension**.\n",
        "\n",
        "## What to Embed: Entire Input Text or Individual Words?\n",
        "\n",
        "If we want to pass `The cat sat` to the LLM, should we convert the entire text to a single embedding vector, or should we convert each word to its own embedding vector? This depends on the type of LLM being used, but most generative LLMs use *word-level* embeddings rather than sentence-level embeddings. So `The cat sat` would be converted to 3 embedding vectors, one for each word. Thus, as input, a generative LLM receives a list of embedding vectors, and as output it returns a list of 6 embedding vectors representing the generated text `on the mat and started playing` (where each word was generated auto-regressively, one by one, internally).\n",
        "\n",
        "However, things are not quite that simple:\n",
        "- First, spaces are not ignored as we might assume from word-level conversion\n",
        "- Second, it's not actually *word-level* embeddings that LLMs use, but *token-level* subword embeddings. Here, a *token* can represent a word, subword, space, special character, or regular character\n",
        "\n",
        "## Why Tokens Instead of Words?\n",
        "\n",
        "Words are usually composed of root words and affixes. If we create an embedding vector for each complete word, we would need a very large number of embedding vectors. For example, `help`, `helper`, `helpless`, `helpful`, and `unhelpful` are 5 different words derived from the root word `help`.\n",
        "\n",
        "Instead, if we split these words into subword tokens like `help`, `er`, `less`, `ful`, and `un`, we still have 5 embedding vectors, but now these subwords can be reused to represent many other words. This significantly reduces the number of embedding vectors needed for the training dataset. These subword units—`help`, `er`, `less`, `ful`, `un`—are called **tokens**.\n",
        "\n",
        "## Token Vocabulary\n",
        "\n",
        "If we collect all the unique tokens from the training dataset, we have a token **vocabulary**. Each token in the vocabulary is assigned a unique identifier called a **token ID**.\n",
        "\n",
        "## Tokenizer: Converting Between Text and Token IDs\n",
        "\n",
        "- A **tokenizer** converts text to token IDs. These token IDs are later converted to embedding vectors by a separate mechanism. LLMs use these input embedding vectors to generate a list of output embedding vectors, which are then converted back to token IDs. The **tokenizer** converts these output token IDs back into text.\n",
        "- The process of converting text to token IDs is called **encoding**, while converting output token IDs back to text is called **decoding**. A **tokenizer** is capable of performing both encoding and decoding."
      ],
      "metadata": {
        "id": "WW0Tz_sFoWAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Training Dataset\n",
        "\n",
        "Before diving deeper into how to convert text to tokens and then tokens to embedding vectors so we can pass them to an LLM, let's first load our dataset.\n",
        "\n",
        "For building our LLM, we will use a very small dataset: a short story by **Edith Wharton** called **The Verdict**.\n",
        "\n",
        "Download the file content from https://en.wikisource.org/wiki/The_Verdict and save it as a file named **the-verdict.txt**."
      ],
      "metadata": {
        "id": "wwHL75-C1tJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the file and read content\n",
        "\n",
        "with open('the-verdict.txt', 'r', encoding='utf-8') as file:\n",
        "  raw_text = file.read()\n",
        "\n",
        "print(f'Total number of characters: {len(raw_text)}')\n",
        "print(raw_text[:99])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "pAnur0hJ3Xlk",
        "outputId": "3f10b95d-412c-4e0c-d95f-dff2404bacff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'the-verdict.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3486255493.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the file and read content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the-verdict.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the-verdict.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple token vocabulary - Word based\n",
        "\n",
        "- Let's implement a simple token vocabulary. We will consider each word as a token. No fancy splitting the words.\n",
        "- As special characters like are space, comma, punctuation mark, exclamation mark are attached to words, lets split by those to get these special characters as separate tokens."
      ],
      "metadata": {
        "id": "to627pbyHwau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple token vocab - v1 - word based\n",
        "\n",
        "import re\n",
        "\n",
        "tokens = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "vocab = sorted(set([token for token in tokens if token.strip()]))"
      ],
      "metadata": {
        "id": "Y4CyYuRfIc3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tokenizer - v1 - Word Based\n",
        "\n",
        "- Let's implement a simple text tokenizer called `SimpleTokenizerV1` using the simple word based token vocabulary we created earlier.\n",
        "- In `encode`, we will split the text into list of token ids. For `split`, we will use the same regex we used to create the vocab for consistency.\n",
        "- In `decode`, **TODO**\n",
        "\n",
        "**Limitations:**\n",
        "-  As the vocab contains only the words from the training dataset, this tokenizer will throw *error when it gets a unknown word* during inference.\n",
        "- If text had `It's the last he painted.`. The encoding and then decoding will return ` It' s the last he painted`. One extra space at the start of the decoded text and one extra space before the `s` of `It's`. This is because of the regex split and join logic we have."
      ],
      "metadata": {
        "id": "bD3XWtaZ6eb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tokenizer v1 - word are tokens here\n",
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.token_to_id = {token: id for id, token in enumerate(vocab)}\n",
        "    self.id_to_token = {id: token for token, id in self.token_to_id.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    token_ids = [self.token_to_id[token] for token in tokens if token.strip()]\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self, token_ids):\n",
        "    tokens = [self.id_to_token[token_id] for token_id in token_ids]\n",
        "    text = ' '.join(tokens)\n",
        "    # Joining by space caused space before each special character as well. Remove it.\n",
        "    text = re.sub(r'\\s+([,.?_!\"()\\']|--)', r'\\1', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "zFW4lUbpF7_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the simple tokenizer v1.\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text_to_encode = 'It\\'s the last he painted.'\n",
        "token_ids = tokenizer.encode(text_to_encode)\n",
        "print(f'Encoded token ids for {text_to_encode}: {token_ids}')\n",
        "\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(f'Decoded text for {token_ids}: {decoded_text}')"
      ],
      "metadata": {
        "id": "on4B4FmiPE0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tokenizer - v2 - Handle unknown words\n",
        "\n",
        "Previous version threw error when it got unknown"
      ],
      "metadata": {
        "id": "6jU7BOTCXakj"
      }
    }
  ]
}