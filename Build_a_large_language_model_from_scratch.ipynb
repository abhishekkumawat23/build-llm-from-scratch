{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyO7eOChMLqWUiMPWPEtFgov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekkumawat23/build-llm-from-scratch/blob/main/Build_a_large_language_model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Large Language Model from Scratch\n",
        "\n",
        "This notebook follows the [Build a Large Language Model (From Scratch)](https://sebastianraschka.com/llms-from-scratch/) book to build a large language model from scratch.\n",
        "\n",
        "Wherever needed, the notebook explains the relevant theory and underlying concepts, focusing on learning through hands-on implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## What is an LLM (Large Language Model)?\n",
        "\n",
        "**Large Language Models (LLMs)** can be of various types, but the most common ones are **Generative LLMs**. A Generative LLM is a deep neural network model that, when given a piece of text (whether a word, line, or paragraph), generates the next word (or token, to be precise) that should follow that text.\n",
        "\n",
        "### Example\n",
        "\n",
        "- Given `The cat sat`, the LLM might generate `on`\n",
        "- Given `The cat sat on`, the LLM might generate `the`\n",
        "- Given `The cat sat on the`, the LLM might generate `mat`\n",
        "\n",
        "Since the model can generate the next word after any given text, if we pass `The cat sat` and call the model multiple times—each time appending the word it generated in the previous iteration—we can generate a significant chunk of text. For example, starting with just `The cat sat`, we can generate `The cat sat on the mat and started playing.`\n",
        "\n",
        "### How It Works\n",
        "\n",
        "- `The cat sat` → `The cat sat on`\n",
        "- `The cat sat on` → `The cat sat on the`\n",
        "- `The cat sat on the` → `The cat sat on the mat`\n",
        "- `The cat sat on the mat` → `The cat sat on the mat and`\n",
        "- `The cat sat on the mat and` → `The cat sat on the mat and started`\n",
        "- `The cat sat on the mat and started` → `The cat sat on the mat and started playing`\n",
        "\n",
        "End users don't need to call the model repeatedly after each word—the LLM wraps this logic internally, calling the underlying model multiple times and appending each generated word to the next iteration. This is why such LLMs are also called **Auto-Regressive** models.\n",
        "\n",
        "---\n",
        "\n",
        "### TODO\n",
        "Introduce additional foundational concepts such as:\n",
        "- Deep neural networks\n",
        "- Transformer layers\n",
        "- Attention mechanisms\n",
        "- Encoder-only, decoder-only, and encoder-decoder architectures\n",
        "- Tokenizers\n",
        "- Embeddings\n",
        "- Difference between inference and training\n",
        "- During training, the entire input is processed at once, while during inference, text is generated one token at a time\n",
        "- KV caching\n",
        "\n",
        "*Note: Some of these concepts may be introduced at later stages in the notebook.*"
      ],
      "metadata": {
        "id": "JXR6lT8iikkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Text Processing\n",
        "\n",
        "## What is an Embedding?\n",
        "\n",
        "LLMs don't understand text the way humans do—they understand numbers. Therefore, we need to:\n",
        "- Convert the input text into a numerical representation before passing it to the LLM\n",
        "- When the LLM predicts the next word, convert its numerical output back into text\n",
        "\n",
        "This numerical representation of text is called an **embedding**. An embedding is not a single number, but rather a vector (a list) of floating-point numbers. The length of this vector depends on the LLM architecture and can be 32, 64, 768, 4096, or more. The length of the embedding vector is called the **embedding dimension**.\n",
        "\n",
        "## What to Embed: Entire Input Text or Individual Words?\n",
        "\n",
        "If we want to pass `The cat sat` to the LLM, should we convert the entire text to a single embedding vector, or should we convert each word to its own embedding vector?\n",
        "\n",
        "This depends on the type of LLM being used, but most generative LLMs use individual embeddings for each word-like unit rather than sentence-level embeddings. So `The cat sat` would be converted to 3 embedding vectors, one for each word. As input, a generative LLM receives a sequence of embedding vectors, and as output it returns another sequence of embedding vectors representing the generated text `on the mat and started playing` (where each word was generated auto-regressively, one by one, internally).\n",
        "\n",
        "However, the reality is more nuanced: LLMs don't actually use *word-level* embeddings, but rather *token-level* subword embeddings. Here, a *token* can represent a complete word, a subword, a space, a special character, or a regular character\n",
        "\n",
        "## Why Tokens Instead of Words?\n",
        "\n",
        "Words are typically composed of root words combined with prefixes and suffixes (affixes). If we created an embedding vector for each complete word, we would need an enormous number of embedding vectors. For example, `help`, `helper`, `helpless`, `helpful`, and `unhelpful` are 5 different words all derived from the root word `help`.\n",
        "\n",
        "Instead, if we split these words into reusable subword tokens like `help`, `er`, `less`, `ful`, and `un`, we only need 5 embedding vectors—but now these subwords can be reused to represent many other words (like `teach`+`er`, `hope`+`less`, `use`+`ful`, `un`+`done`). This significantly reduces the total number of embedding vectors needed for the training dataset. These reusable subword units—such as `help`, `er`, `less`, `ful`, and `un`—are called **tokens**.\n",
        "\n",
        "## Token Vocabulary\n",
        "\n",
        "If we collect all the unique tokens from the training dataset, we have a token **vocabulary**. Each token in the vocabulary is assigned a unique identifier called a **token ID**.\n",
        "\n",
        "## Tokenizer: Converting Between Text and Token IDs\n",
        "\n",
        "A **tokenizer** is responsible for converting between text and token IDs:\n",
        "- **Encoding**: The tokenizer converts text into token IDs. These token IDs are then converted to embedding vectors by the embedding layer. The LLM processes these input embedding vectors and generates output embedding vectors, which are converted back to token IDs.\n",
        "- **Decoding**: The tokenizer converts the output token IDs back into human-readable text.\n",
        "\n",
        "A tokenizer is capable of performing both encoding and decoding operations.\n",
        "\n",
        "## Embedding vs Token IDs\n",
        "\n",
        "One might think that as we can get token IDs from tokens, we already got a numerical representation of tokens. Then what's the need of converting them to embedding vectors? Why not just pass the token ids (numerical representation) to LLMs?\n",
        "\n",
        "Emebdding vectors are more than just numerical reprsentations. They somehow also contains relationships and pattern between words. Similar embedding vectors are near to each other, and dissimilar are far. For example, embedding vectors of *cat*, *dog* will be near to each other than *car*. By *near*, I mean angle between *cat* and *dog* embedding vectors is less than angle between between *cat* and *car* in the embedding vector space. In other words, emebdding vectors holds semantic relationship between words. We will cover this later on that how emebdding vectors holds these semantic relationships and complex patterns. For now, we just wanted to state that we still need embedding vectors even though we have token ids as numerical representation of tokens."
      ],
      "metadata": {
        "id": "WW0Tz_sFoWAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Loading the Training Dataset\n",
        "\n",
        "Before diving deeper into converting text to tokens and then tokens to embedding vectors for input to an LLM, let's first load our dataset.\n",
        "\n",
        "To build our LLM, we will use a very small dataset: a short story by **Edith Wharton** titled **The Verdict**.\n",
        "\n",
        "Download the text from https://en.wikisource.org/wiki/The_Verdict and save it as a file named **the-verdict.txt**."
      ],
      "metadata": {
        "id": "wwHL75-C1tJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the file and read content\n",
        "\n",
        "with open('the-verdict.txt', 'r', encoding='utf-8') as file:\n",
        "  raw_text = file.read()\n",
        "\n",
        "print(f'Total number of characters: {len(raw_text)}')\n",
        "print(raw_text[:99])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAnur0hJ3Xlk",
        "outputId": "60383ff5-6d11-4649-d657-43acc7f0874d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Token Vocabulary - Word-Based\n",
        "\n",
        "- Let's implement a simple token vocabulary. We will treat each word as a token, without any fancy word splitting.\n",
        "- Since special characters like spaces, commas, periods, and exclamation marks are typically attached to words, let's split by these characters to extract them as separate tokens."
      ],
      "metadata": {
        "id": "to627pbyHwau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple token vocab - v1 - word based\n",
        "\n",
        "import re\n",
        "\n",
        "tokens = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "vocab = sorted(set([token for token in tokens if token.strip()]))"
      ],
      "metadata": {
        "id": "Y4CyYuRfIc3J"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tokenizer - v1 - Word-Based\n",
        "\n",
        "- Let's implement a simple text tokenizer called `SimpleTokenizerV1` using the word-based token vocabulary we created earlier.\n",
        "- In the `encode` method, we will convert the text into a list of token IDs. For splitting the text, we will use the same regex pattern used to create the vocabulary, ensuring consistency.\n",
        "- In the `decode` method, **TODO**\n",
        "\n",
        "**Limitations:**\n",
        "- Since the vocabulary contains only words from the training dataset, this tokenizer will **throw an error when it encounters an unknown word** during inference.\n",
        "- Given text like `It's the last he painted.`, the encoding and subsequent decoding will return ` It' s the last he painted.` with an extra space at the start of the decoded text and an extra space before the `s` in `It's`. This occurs due to how the regex splits tokens and how they are rejoined during decoding."
      ],
      "metadata": {
        "id": "bD3XWtaZ6eb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tokenizer v1 - word are tokens here\n",
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.token_to_id = {token: id for id, token in enumerate(vocab)}\n",
        "    self.id_to_token = {id: token for token, id in self.token_to_id.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    token_ids = [self.token_to_id[token] for token in tokens if token.strip()]\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self, token_ids):\n",
        "    tokens = [self.id_to_token[token_id] for token_id in token_ids]\n",
        "    text = ' '.join(tokens)\n",
        "    # Joining by space caused space before each special character as well. Remove it.\n",
        "    text = re.sub(r'\\s+([,.?_!\"()\\']|--)', r'\\1', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "zFW4lUbpF7_A"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the simple tokenizer v1.\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text_to_encode = 'It\\'s the last he painted.'\n",
        "token_ids = tokenizer.encode(text_to_encode)\n",
        "print(f'Encoded token ids for {text_to_encode}: {token_ids}')\n",
        "\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(f'Decoded text for {token_ids}: {decoded_text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on4B4FmiPE0d",
        "outputId": "0a9e5ad9-b520-488d-f0cc-45d4ffa06392"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded token ids for It's the last he painted.: [58, 2, 872, 1013, 615, 541, 763, 7]\n",
            "Decoded text for [58, 2, 872, 1013, 615, 541, 763, 7]: It' s the last he painted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tokenizer - v2 - Word Based\n",
        "\n",
        "**Adding the `<|unk|>` token:** The previous version threw an error when it encountered unknown words. We can introduce a special token to represent any unknown word. Let's call this token `<|unk|>`.\n",
        "\n",
        "**Adding the `<|endoftext|>` token:** As we know, we pass a list of token IDs to the LLM. The maximum length of this list is called `max_seq_len`. We will later learn that computationally, it costs roughly the same whether we pass 1 token ID or `max_seq_len` token IDs to the LLM. Since the computational cost is similar, it makes sense during training to always pass inputs of length `max_seq_len` to maximize efficiency.\n",
        "\n",
        "To achieve this, we can concatenate multiple lines or sentences until we reach `max_seq_len`, then pass that combined input to the LLM. However, we need the LLM to treat these concatenated lines as separate and unrelated, so that text generation isn't negatively affected by the arbitrary concatenation. For this purpose, we can add a special token called `<|endoftext|>` between concatenated lines. We can then train the LLM to understand that segments separated by `<|endoftext|>` are independent and unrelated to each other."
      ],
      "metadata": {
        "id": "6jU7BOTCXakj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add `<|unk|>` and `<|endoftext|>` tokens in the vocab\n",
        "vocab.extend(['<|unk|>', '<|endoftext|>'])\n",
        "print(vocab[-5:])\n",
        "\n",
        "# Create tokenize with these special tokens support\n",
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.token_to_id = {token:id for id, token in enumerate(vocab)}\n",
        "    self.id_to_token = {id:token for id, token in enumerate(vocab)}\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    tokens = [token for token in tokens if token.strip()]\n",
        "    tokens = [token if token in self.token_to_id else '<|unk|>' for token in tokens]\n",
        "    token_ids = [self.token_to_id[token] for token in tokens]\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self, token_ids):\n",
        "    tokens = [self.id_to_token[token_id] for token_id in token_ids]\n",
        "    text = ' '.join(tokens)\n",
        "    text = re.sub(r'\\s+([,.?_!\"()\\']|--)', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-sKDVfS97Me",
        "outputId": "9441bdd1-1dcf-4906-9b58-f231035ccaba"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['younger', 'your', 'yourself', '<|unk|>', '<|endoftext|>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets test with tokenizer\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "token_ids = tokenizer.encode('Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.')\n",
        "print(tokenizer.decode(token_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKRUpxuc-yLz",
        "outputId": "5ce588b6-d393-4e7b-dd23-5c4dc5faae1c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Simple Byte based Tokenizer\n",
        "\n",
        "**Limitations of simple tokenizer:**\n",
        "\n",
        "In the simple tokenizers we implemented, there are many limitations:\n",
        "- We are considering words as tokens but as we discussed earlier example of `help`, `helper`, `helpless`, `helpful`, and `unhelpful` where we if instead of these, we stored `help`, `er`, `less`, `ful` and `un` as tokens, we would end up storing way few tokens as these common prefix and suffix we separated can be used to create many other words.\n",
        "- Even though we are handling unknown word with special token `<|unk|>` and not erroring out, we are not passing the actual unkown word to the LLM, and thus even if LLM know how to make predictions with the unknown words(Example: typos), it can't do anything as its not getting that unknown word. Thus, Tokenizer should be able to create tokens for unknown words.\n",
        "\n",
        "**Tokenizer made out of byte tokens:**\n",
        "\n",
        "As we know that at the very core, each word, character, symbol, special tokens are made out of bytes. And how many bytes are possible? 256 (2^8 different values). So if we create a token vocabulary with all these 256 bytes in it, we can convert any word, character, symbol to tokens. This way we don't need to think about any unknown words.\n",
        "\n",
        "**Do we need to implement byte tokenizer?:**\n",
        "\n",
        "Python text already has encode and decode methods which encodes text to bytes and then decodes to text, so we don't need to implement a byte tokenizer. But lets still implement one because we are going to make it more complicated by adding many optimization in it.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "In below tokenizer, number of tokens are only 256 so very small number of token. In addition, there is no problem of unknown words as all words can be created from the 256 bytes. There what are the limitations?\n",
        "- Number of tokens required to represent a single word will be huge. Even a simple english word like `implement` now needs 9 tokens just to reprsent 1 word. LLMs have a limit on number of tokens it can take called `max_seq_length`. As we have increased number of tokens needed per word, LLM can take way smaller text/words in this case.\n",
        "- As we have only 256 tokens, corresponding emebdding vectors will only be 256. As we know that embedding vectors somehow holds semnatic relationships as well, as we now have only 256 such vectors, the amount of relationships it can hold will be limited."
      ],
      "metadata": {
        "id": "1KDeCTflC8si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleByteTokenizer:\n",
        "  def __init__(self):\n",
        "    self.token_to_id = {bytes([id]): id for id in range(256)}\n",
        "    self.id_to_token = {id: token for token, id in self.token_to_id.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    tokens = text.encode('utf-8') # Using this method, we get the bytes\n",
        "    token_ids = [self.token_to_id[bytes([token])] for token in tokens]\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self, token_ids):\n",
        "    tokens = [self.id_to_token[token_id] for token_id in token_ids]\n",
        "    all_bytes = b''.join(tokens)\n",
        "    return all_bytes.decode('utf-8', errors='replace')"
      ],
      "metadata": {
        "id": "LAAudTSYK2Y3"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleByteTokenizer()\n",
        "token_ids = tokenizer.encode('Hello how are you?')\n",
        "print(token_ids)\n",
        "print(tokenizer.decode(token_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvYF7Tx2MgUM",
        "outputId": "4f8f1c1f-e48c-4047-bc40-9454b928b244"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 101, 108, 108, 111, 32, 104, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63]\n",
            "Hello how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Byte Pair Encoding Tokenizer\n",
        "\n",
        "We can build on top of our existing simple byte tokenizer to address its limitations. We are going to build a tokenizer called **Byte Pair Encoding** Tokenizer aka **BPE** Tokenizer. GPT like LLMs use this BPE tokenizer. Here for simplicity we will implement a simple version.\n",
        "\n",
        "**Creating the vocab:**\n",
        "- Go through entire training dataset and split it into bytes to get `tokens`.\n",
        "- Create a frequency map of consecutive byte pairs. Find out the byte pair with max frequency.\n",
        "- Merge this pair and add the merged bytes in the `token_to_id` and `id_to_token` map.\n",
        "- Also go through the `tokens` list and replace the byte pair with single merged bytes.\n",
        "- Now repeat this process until you reach a vocab of size you want. For example if you want vocab of 5000 size, then repeat the process for 5000-256 times as we already had 256 values in the vocab.\n",
        "- Repeating this process allows vocab to be added with most common sub-words, word (in merged byte format).\n",
        "\n",
        "**Encoding:**\n",
        "- Apply greedy longest match approach to split text into minimal tokens within O(n) time\n",
        "\n",
        "**Decoding:**\n",
        "- Decoding is pretty simple as same as before."
      ],
      "metadata": {
        "id": "00S5cHf7ldo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class SimpleBPETokenizer:\n",
        "  def __init__(self, vocab_size, freq_threshold):\n",
        "    self.token_to_id = {bytes([id]): id for id in range(256)}\n",
        "    self.id_to_token = {id: token for token, id in self.token_to_id.items()}\n",
        "    self.vocab_size = vocab_size\n",
        "    self.freq_threshold = freq_threshold\n",
        "\n",
        "  def train(self, text):\n",
        "    # Pass entire training text as text so that we can create tokens for common\n",
        "    # sub-words, words etc.\n",
        "    tokens = text.encode('utf-8')\n",
        "    tokens = [bytes([token]) for token in tokens]\n",
        "\n",
        "    num_merges = self.vocab_size - len(self.token_to_id)\n",
        "    for _ in range(num_merges):\n",
        "      # Find best token pair which comes together most frequently.\n",
        "      pairs = Counter()\n",
        "      for idx in range(len(tokens)-1):\n",
        "        pair = (tokens[idx], tokens[idx+1])\n",
        "        pairs[pair] += 1\n",
        "      if not pairs:\n",
        "        # Note: Train text had very few tokens that we have exhausted them\n",
        "        # before creating the vocab of needed size.\n",
        "        break\n",
        "      best_pair = max(pairs, key=pairs.get)\n",
        "      best_pair_merged = best_pair[0] + best_pair[1]\n",
        "\n",
        "      # If best_pair's frequency doesn't cross the frequency threshold, then stop.\n",
        "      if pairs[best_pair] < self.freq_threshold:\n",
        "        break\n",
        "\n",
        "      # Add best pair in our token_to_id and id_to_token maps.\n",
        "      best_pair_idx = len(self.token_to_id) # As we are adding as last, index will be last.\n",
        "      self.token_to_id[best_pair_merged] = best_pair_idx\n",
        "      self.id_to_token[best_pair_idx] = best_pair_merged\n",
        "\n",
        "      # Merge this best pair in tokens so that next iteration can repeat the\n",
        "      # process but where tokens have this best pair merged. This way, iteration\n",
        "      # by iteration, we would have merged common sub-words, words, etc.\n",
        "      new_tokens = []\n",
        "      idx = 0\n",
        "      while idx < len(tokens):\n",
        "        if idx < len(tokens) - 1 and (tokens[idx], tokens[idx+1]) == best_pair:\n",
        "          new_tokens.append(best_pair_merged)\n",
        "          idx += 2\n",
        "        else:\n",
        "          new_tokens.append(tokens[idx])\n",
        "          idx += 1\n",
        "      tokens = new_tokens\n",
        "\n",
        "  def vocab(self):\n",
        "    return list(self.token_to_id.keys())\n",
        "\n",
        "  def encode(self, text):\n",
        "    # Apply greedy longest match approach to split text into minimal tokens within O(n) time\n",
        "    # TODO: is there stringbuilder kinda thing in python?\n",
        "    text_bytes = [bytes([token]) for token in text.encode('utf-8')]\n",
        "\n",
        "    start = 0\n",
        "    end = 0\n",
        "    current_token = b''\n",
        "    tokens = []\n",
        "    while end <= len(text_bytes):\n",
        "      if end == len(text_bytes):\n",
        "        tokens.append(current_token)\n",
        "        break\n",
        "      new_token = current_token + text_bytes[end]\n",
        "      if new_token in self.token_to_id:\n",
        "        current_token = new_token\n",
        "      else:\n",
        "        tokens.append(current_token)\n",
        "        # print(f'Tokens: {tokens}')\n",
        "        start = end\n",
        "        current_token = text_bytes[end]\n",
        "      end += 1\n",
        "\n",
        "    token_ids = [self.token_to_id[token] for token in tokens]\n",
        "    return token_ids\n",
        "\n",
        "  def decode(self, token_ids):\n",
        "    tokens = [self.id_to_token[token_id] for token_id in token_ids]\n",
        "    all_bytes = b''.join(tokens)\n",
        "    return all_bytes.decode('utf-8', errors='replace')"
      ],
      "metadata": {
        "id": "zx0j3m30o-_W"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train tokenizer\n",
        "vocab_size = 500\n",
        "freq_threshold = 5\n",
        "tokenizer = SimpleBPETokenizer(vocab_size, freq_threshold)\n",
        "# print(raw_text[:99])\n",
        "tokenizer.train(raw_text)\n",
        "\n",
        "# Print vocab\n",
        "vocab = tokenizer.vocab()\n",
        "print(vocab[-10:])\n",
        "print(f'Vocab size: {len(vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtEzF7wLqFGZ",
        "outputId": "99ea3344-bca9-41ef-e22e-b5c281b4bc6d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'for ', b'ould', b'tch', b'gre', b'pr', b'ould have ', b'--t', b'cou', b'ure ', b'k ']\n",
            "Vocab size: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try encoding and decoding\n",
        "token_ids = tokenizer.encode('I felt able to face the fact')\n",
        "print(token_ids)\n",
        "print(tokenizer.decode(token_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JHKWOmymdDj",
        "outputId": "71e30178-dc8d-4cf7-c019-a6d12d54c5e5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[278, 102, 288, 259, 336, 353, 116, 273, 102, 300, 305, 262, 102, 300, 116]\n",
            "I felt able to face the fact\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT's BPE Tokenizer\n",
        "\n",
        "The tokenizer we wrote just gives a simple idea about Byte Pair Encoding works. It lacks lot of optimizations like:\n",
        "- We don't want weird tokens spreading over spaces. So ideally we should have pre-tokenize using word split to create text chunks, and then built the byte pair encoding on each chunk.\n",
        "- Our encode is not efficient. Even though we are using greedy approach, we are appending to string multiple times where string is immutable. This causes time complexity to increase.\n",
        "\n",
        "These are just few missing points we discussed but there are many. So, its better to use **GPT's BPE tokenizer**. Alternatively, we can also use some other tokenizers like **SentencePiece** tokenizer."
      ],
      "metadata": {
        "id": "3ZL7Ti0vxfJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "token_ids = tokenizer.encode('I felt able to face the fact')\n",
        "print(token_ids)\n",
        "print(tokenizer.decode(token_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT9YrnH4zG3a",
        "outputId": "383edcfc-f2f8-4c61-925b-4c60307cef98"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40, 2936, 1498, 284, 1986, 262, 1109]\n",
            "I felt able to face the fact\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Data Loader\n",
        "\n",
        "To prepare data for LLM training, we need to a lot of things:\n",
        "- Data sampling - i.e. Creating chunks of entire raw text so that each chunk can be passed as one input sequence to the LLM. Here we can use stride and sliding window concept to pick the samples.\n",
        "- Creating input-target pairs\n",
        "- Creating batches of inputs so that one entire batch can be passed to the LLM at a time\n",
        "- Whether to shuffle data or not betweeb say epochs\n",
        "- Whether to drop the last batch or not as it might contain less data and thus we can avoid unnecessary spikes.\n",
        "\n",
        "We can write all these logics by ourselves and its not that difficult but we have to be cautius while sampling the data.\n",
        "\n",
        "Instead, we can use torch's dataset and dataloader classes to define the dataset."
      ],
      "metadata": {
        "id": "eK4IDbzk0Pft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)"
      ],
      "metadata": {
        "id": "bDrYz4Gj2WL6"
      },
      "execution_count": 85,
      "outputs": []
    }
  ]
}